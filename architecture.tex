\chapter{\MyTitle}
\label{architecture}

This chapter presents an overview of a garment-agnostic algorithm to unfold clothes using depth sensor data, that will be extended in later chapters. The pipeline for folding clothes followed throughout robotic literature is based on how it is performed by humans. This allows these tasks to be executed in the same environments as humans, aiming towards a fully-automated or collaborative fashion. Garments are extracted from the washing or drying machine forming a pile, and an iterative process begins. First, an individual clothing article is picked from the pile. Then, the garment is extended in the air or assisted by a flat surface, during which unfolding and wrinkle removing procedures may be performed for aiding the posterior classification and manipulation of the garment. A classification procedure is applied to fit clothing article within a certain garment category. Finally, a standard manipulation sequence specific to its category is applied to fold the garment for storage. This iterative process is repeated until there are no clothes left on the pile.

Our work differs with most of the state of the art in the fact that garment models are not used. Our approach uses depth information captured with an RGB-D sensor to detect folds in a garment. The most suitable grasping position, unfolding direction, and release point are computed.

The algorithm can be divided into three main stages, shown in Figure \ref{fig:pipeline_block_diagram}. First, the  background is extracted from the image and the contour of the garment is approximated. Then, a depth map clustering process is performed to estimate overlapping regions. Finally, the information obtained in the previous process is used to determine if there are any folds present and how to interact with the garment to unfold it. The whole unfolding pipeline is an iterative process, that should be repeated again until the garment is completely extended.

The algorithm has been open sourced and is available online\footnote{\url{https://github.com/roboticslab-uc3m/textiles}}.

\begin{figure}[thpb]
    \centering
    \includegraphics[width=\textwidth]
    {figures/Main_diagram.pdf}
    \caption{Garment Unfolding pipeline. The pipeline uses RGB and depth images captured from the robot sensors as inputs and process them in 3 consecutive stages. This process is iterative, and it is applied until the garment is fully unfolded.}
    \label{fig:pipeline_block_diagram}
\end{figure}

\section{Garment Segmentation}
\label{architecture:garment_segmentation}

The objective of this garment segmentation stage is to substract the background from the input RGB-D image to obtain only the resulting garment data, as well as to extract the simplified garment contour. Figure \ref{fig:generic_garment_segmentation} depicts a generic sample of this background subtraction and contour extraction process.


%\begin{figure}[thpb]
%    \centering
%    \includegraphics[width=0.8
%    \textwidth]{figures/placeholder2.png}
%    \caption{\comment{A nice figure of a generic Background subtraction and contour extraction}}
%    \label{fig:generic_garment_segmentation}
%\end{figure}


\begin{figure}[htbp]
	\centering
    \begin{subfigure}[l]{0.49\textwidth}
        \centering
    	\includegraphics[width=\textwidth]
    	{figures/Background-segmentation-01.png}
        \end{subfigure}
        ~
        \begin{subfigure}[r]{0.49\textwidth}
	        \centering
    		\includegraphics[width=\textwidth]
    		{figures/Background-segmentation-02.jpg}
		\end{subfigure} 
    \caption[dummy]{Two examples of background segmentation. In the left image \cite{ren2013image}, the foreground objects contour is highlighted in red. In the right image\cite{nieuwenhuis-cremers-pami12_2}, the foreground object (the tiger) is highlighted in red, and the background is highlighted in green.}
    \label{fig:generic_garment_segmentation}
\end{figure}


Prior to any further analysis, a background subtraction step has to be performed in order to remove all information not related to the garment. Based on the assumption that the clothing article is placed on top of a flat white surface, the garment can be segmented by color. This can be performed by threshold operations on the red, green and blue channels of the RGB space, or on the hue, saturation and value channels of the HSV space. Other color spaces could also be used. GrabCut \cite{rother2004grabcut}, that uses gaussian mixture models (GMMs) and iterative energy minimization, is another available method to separate the background and foreground based on color.

Another potential approach is to remove the horizontal plane of the table using depth information. This approach can be further improved by combining the results obtained by the depth-based segmentation with a color segmentation, in case the garment color and the background are dissimilar enough.

Once the background is extracted, the contour of the garment has to be determined. Several methods exist for this purpose. The Topological Analysis by Border Following algorithm developed by Suzuki and Abe \cite{suzuki1985topological} is a widely used algorithm for connected-component labeling and countour finding. A Marching Squares algorithm, a variation of the Marching Cubes algorithm \cite{lorensen1987marching}, can be also applied to find constant valued contours in an image.

After the contour is extracted, it is then approximated to a polygon, e.g. using the Ramer-Douglas-Peucker algorithm \cite{ramer1972iterative, douglas1973algorithms}. Alternative algorithms exist, such as the Teh-Chin algorithm to find Dominant Points \cite{teh1989detection}. Each of the segments obtained is a candidate for being a fold axis. The midpoints of those fold axis candidates are calculated to be used in the following steps.

The garment segmentation process used in this thesis is further explained in chapter \ref{garment_segmentation}.

\section{Garment Depth Map Clustering}
\label{architecture:depth_map_clustering}
When the garment has been identified, garment regions with similar height points must be clustered and labeled. By using only depth information, as opposed to using RGB images, makes our algorithm independent of the colors and patterns present in the garments. Many algorithms exist to perform this clustering step.

For general purpose clustering over any set of unlabeled data we can find several algorithms that can be also applied to images. K-means, Mean Shift \cite{comaniciu2002mean} or DBSCAN \cite{ester1996density} are some examples of clustering algorithms that can be applied to group pixels with similar depth.

A particular case of clustering algorithms are those which are applied to obtain superpixels. A superpixel \cite{ren2003learning} is a local region of the image with similar characteristics, such as color, texture, etc. Several examples of superpixel can be seen in Figure \ref{fig:generic_superpixels}. This reduces the number of pixels to regions that are local and coherent, preserving most of the structure of the diferent items at the scale of interest. Many algorithms exist to perform superpixel clustering, such as SLIC \cite{achanta2010slic}, Quickshift \cite{vedaldi2008quick}, Turbopixels \cite{levinshtein2009turbopixels} and Normalized Cuts \cite{mori2005guiding}.

%\begin{figure}[thpb]
%    \centering
%    \includegraphics[width=\textwidth]{figures/Superpixels-01.png}
%    \caption{\comment{A nice figure of a generic Superpixels}}
%    \label{fig:generic_superpixels}
%\end{figure}


\begin{figure}[htbp]
	\centering
    \begin{subfigure}[l]{0.51\textwidth}
        \centering
    	\includegraphics[width=\textwidth]
    	{figures/Superpixels-02.jpg}
    \end{subfigure}
    ~
    \begin{subfigure}[r]{0.47\textwidth}
	    \centering
   		\includegraphics[width=\textwidth]
    	{figures/Superpixels-03.png}
	\end{subfigure}
	\caption[dummy]{Several examples of Superpixel clustering algorithms. Left image \cite{mori2005guiding} corresponds to a Normalized Cut algorithm with two different average cluster sizes, 100 pixels in the case of the upper left part of the image, and 300 in the lower right part. Right image \cite{achanta2012slic}
	 corresponds to a SLIC (Simple Linear Iterative Clustering) algorithm \cite{achanta2010slic} applied to obtain average sizes of 64, 256 and 1024 pixels.}
    \label{fig:generic_superpixels}
\end{figure}

When working with images, the clustering process of finding regions both compact and with homogeneous characteristics is known as segmentation. Many image segmentation algorithms exist based on similar regions and closed contours. One of these algorithms, widely used, is the Watershed transform algorithm \cite{digabel1978iterative}, which interprets the image as a topological surface, filling the image with water from several point sources until they merge, and creating walls in those intersection contours. The regions isolated by these walls are the segmented regions.

The garment depth map clustering process used in this thesis is developed in more detail in chapter \ref{garment_clustering}.

\section{Garment Pick and Place Points}
\label{garment_PnP_points}

Once we have clustered the depth image in regions of similar height, we can proceed to analyze those regions. The objective is to find which parts of the region outline correspond to the fold line, which is physically connected to the rest of the garment. Once the fold line has been detected, pick and place points have to be found for the robot to perform the unfolding operation. Figure \ref{fig:picking_points} displays an example of this unfolding operation, with its pick and place points marked.

\begin{figure}[thpb]
    \centering
    \includegraphics[width=0.8
    \textwidth]{figures/teo-pick-and-place.pdf}
    \caption{An example of an unfolding operation. The figure shows the humanoid robot manipulating a garment. The folded part that is being manipulated, the sleeve, is highlighted with a green dashed line. The green arrow line represents the unfolding trajectory, with the pick and place points at the beginning and the end of it, respectively.}
    \label{fig:picking_points}
\end{figure}


If grouping of certain segmented regions is required, i.e. there are small superpixels with similar characteristics, a merging algorithm has to be performed. A graph-based approach could be used for both merging clusters and determining which clusters are physically connected, either by the fold line or because they belong to the same garment region.


Once the overlapping fold is identified, and the fold line is known, we select pick and place points for performing the unfold operation. This operation, that will be executed by a robot, has to take the garment from the original state to a more ``unfolded'' state.

Several choices exist for pick points, as well as for place points. To pick the garment, one can assume that the highest points in the overlapped region are good candidates, as they probably are easier for the robot to grasp, and they will be separated from the underlying garment. Other potential choices are the points that belong to the border of the overlapping region, except for those which belong to the fold line. These approaches are typically followed by humans when performing unfolding tasks. For place points, the points should be far enough from the garment to completely unfold the overlapping region, but not far enough for the garment to be dragged with the movement. It is also convenient to choose points that lie on the oposite side of the folding line with respect to the pick point, otherwise is almost impossible to perform the unfold operation.