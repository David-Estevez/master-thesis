\section{Experiments}
\label{experiments}

The final experiments using the current algorithm have been performed using an ASUS Xtion PRO LIVE at 640x480 RGB and depth streams (30 fps).
To simplify the image analysis, the sensor is placed on top of the working surface  providing a bird's eye view over the cloth folding environment, with its image plane almost parallel to the working surface.

In the present work we are using thick blankets and towels, to cope with the limitations of the resolution of the depth sensor. A set of six samples with one or two folds were presented to the system for analysis. The results are shown in Fig. \ref{directions}.
%
These results show that the algorithm generates acceptable unfolding directions.

%However, when analyzing the experimental results, one can see that the candidate paths for unfolding are created using the garment contour segment midpoint. This fact can warp the most intuitive direction, which would be the closest between the highest region and the garment contour.

According to the results, an interpolation between the best two directions could result in a better solution. The best directions for the set of clothes are shown in Fig. \ref{directions_several}. %The arrow's directions make us believe that a combined directions by averaging them could provide better results.

%\warning{In \cite{Li2015IROS}, Li et al. present a method for folding deformable objects such as clothes. }

%A final demonstration of the current algorithm has been performed using the full-size humanoid robot TEO \cite{martinez2012teo}. Depth sensor coordinates are passed to the robot root frame, and a standard pick and place operation is performed with the given points. Figure \ref{setup} depicts this scenario.